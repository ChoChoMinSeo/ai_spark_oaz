{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from utils.loss_fn import FocalDiceIouLoss, iou_pytorch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'seed':10,\n",
    "    'val_size':0.3,\n",
    "    'channels': [2,4,6],\n",
    "    'experiment':True,\n",
    "    'batch_size':8,\n",
    "    'lr':1e-3,\n",
    "    'epochs':100,\n",
    "    'early_stopping':15,\n",
    "    'inference':{\n",
    "        'threshold':0.25\n",
    "    }\n",
    "}\n",
    "DATAPATH='./dataset'\n",
    "data_paths={\n",
    "    'train_csv': DATAPATH+'/train_meta_exp.csv',\n",
    "    'test_csv': DATAPATH+'/test_meta_exp.csv',\n",
    "    'train_img':DATAPATH+'/train_img/',\n",
    "    'train_mask':DATAPATH+'/train_mask/',\n",
    "    'test_img':DATAPATH+'/test_img/',\n",
    "}\n",
    "train_csv = pd.read_csv(data_paths['train_csv'])\n",
    "SAVE_PATH = './logs'\n",
    "save_paths={\n",
    "    'model':SAVE_PATH+'/models/',\n",
    "    'val':SAVE_PATH+'/vals/'\n",
    "}\n",
    "# 시드 고정 함수\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "fix_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wildfire_Dataset(Dataset):\n",
    "    def __init__(self, csv, transform_f=None,infer=False):\n",
    "        self.csv = csv\n",
    "        self.transform_f = transform_f\n",
    "        self.infer = infer\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_path = self.csv.iloc[idx,0]\n",
    "        # test인 경우 infer==True\n",
    "        if self.infer:\n",
    "            image = rasterio.open(data_paths['test_img']+img_path).read().transpose(1,2,0)[:,:,config['channels']]\n",
    "            image = image/(2**16)\n",
    "            image = image.astype(np.float32)\n",
    "            if self.transform_f:\n",
    "                image = self.transform_f(image=image)['image']\n",
    "            return image\n",
    "        # train인 경우\n",
    "        image = rasterio.open(data_paths['train_img']+img_path).read().transpose(1,2,0)[:,:,config['channels']]\n",
    "        image = image/(2**16)\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        mask_path = self.csv.iloc[idx,1]\n",
    "        mask = rasterio.open(data_paths['train_mask']+mask_path).read().transpose(1,2,0)\n",
    "        if self.transform_f:\n",
    "            augmented = self.transform_f(image=image, mask = mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        return image, mask, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transforms = {\n",
    "    'train': A.Compose([\n",
    "        # A.RandomBrightnessContrast(contrast_limit=(-0.2,-0.2),p=1),\n",
    "        # A.GaussianBlur(),\n",
    "        A.Normalize((0.5, ), (0.5, )),\n",
    "        ToTensorV2(transpose_mask=True)\n",
    "    ]),\n",
    "    'valid':A.Compose([\n",
    "        A.Normalize((0.5, ), (0.5, )),\n",
    "        ToTensorV2(transpose_mask=True)\n",
    "    ]),\n",
    "    'test':A.Compose([\n",
    "        A.Resize(512,512),\n",
    "        A.Normalize((0.5, ), (0.5, )),      \n",
    "        ToTensorV2(transpose_mask=True)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(config['seed'])\n",
    "train_csv_, valid_csv_ = train_test_split(train_csv,test_size=config['val_size'],random_state=config['seed'])\n",
    "train_dataset = Wildfire_Dataset(train_csv_,transform_f=base_transforms['train'],infer=False)\n",
    "valid_dataset = Wildfire_Dataset(valid_csv_,transform_f=base_transforms['valid'],infer=False)\n",
    "# B,C,H,W\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'],shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "print(f'train size: {len(train_dataset)}, valid size: {len(valid_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation Result Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs_to_show=4\n",
    "fig,axs = plt.subplots(num_imgs_to_show,len(config['channels'])+1)\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(20)\n",
    "for i in range(len(config['channels'])):\n",
    "    axs[0,i].set_title('channel'+ str(config['channels'][i]))\n",
    "axs[0,i+1].set_title('label')\n",
    "\n",
    "cnt=0\n",
    "for idx,(imgs, masks,img_idx) in enumerate(train_loader):\n",
    "    for i in range(len(imgs)):\n",
    "        for j in range(len(config['channels'])):\n",
    "            axs[i,j].imshow(imgs[i,j,:,:],cmap='gray')\n",
    "        axs[i,-1].imshow(masks[i,0,:,:],cmap='gray')\n",
    "        axs[i,-1].set_xlabel(str(masks[i,0,:,:].sum().sum().item()))\n",
    "        cnt+=1\n",
    "        if cnt==num_imgs_to_show:\n",
    "            break\n",
    "    if cnt==num_imgs_to_show:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.CWUnet import CWUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(config['seed'])\n",
    "model = CWUnet()\n",
    "torch.save(model.state_dict(), save_paths[\"model\"]+'/base_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(config, model, criterion, valid_loader,epoch,val_pmap):\n",
    "    model.eval()\n",
    "    valid_loss=0\n",
    "    valid_iou = 0\n",
    "    valid_focal = 0\n",
    "    valid_dice = 0\n",
    "    \n",
    "    y_pred_dict={}\n",
    "    with torch.no_grad():\n",
    "        for idx,(images, masks,img_idx) in tqdm(enumerate(valid_loader)):\n",
    "            images = images.to(config['device'],dtype = torch.float32)\n",
    "            masks = masks.to(config['device'],dtype = torch.float32)\n",
    "            \n",
    "            if epoch==0:\n",
    "                for i in img_idx:\n",
    "                    val_pmap[i.item()] = torch.zeros(images.shape[0],1,256,256)\n",
    "                p_map = torch.zeros(images.shape[0],1,256,256).to(config['device'])\n",
    "            else:\n",
    "                p_map= torch.concat([val_pmap[i.item()] for i in img_idx],axis=0).to(config['device'])\n",
    "            input_x = torch.concat([p_map,images],axis=1)\n",
    "            \n",
    "            outputs = model(input_x)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for idx_1,i in enumerate(img_idx):\n",
    "                    val_pmap[i.item()] = torch.sigmoid(outputs[idx_1:idx_1+1,:,:,:])\n",
    "            \n",
    "            \n",
    "            dice_loss,focal_loss,jaccard_loss, loss = criterion(outputs, masks)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            valid_dice += dice_loss.item()\n",
    "            valid_focal += focal_loss.item()\n",
    "            valid_iou += 1-(jaccard_loss.item())\n",
    "            \n",
    "            for i in range(len(outputs)):\n",
    "                y_pred = np.where(outputs[i,0,:,:].to('cpu')>config['inference']['threshold'],1,0).astype(np.uint8)\n",
    "                y_pred_dict[str(idx*config['batch_size']+i)] = y_pred\n",
    "        valid_save_path = save_paths['val']+f'epoch_{epoch}.pkl'\n",
    "        joblib.dump(y_pred_dict,valid_save_path)\n",
    "    return valid_loss/len(valid_loader), valid_iou/len(valid_loader), valid_dice/len(valid_loader),valid_focal/len(valid_loader), val_pmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, train_loader, valid_loader):\n",
    "    model = model.to(config['device'])\n",
    "    es_count = 0\n",
    "    min_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    train_pmap = {}\n",
    "    val_pmap = {}\n",
    "    criterion = FocalDiceIouLoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = config['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-8, verbose=True)\n",
    "    print('***TRAINING START***')\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        epoch_loss=0\n",
    "        epoch_iou = 0\n",
    "        epoch_dice=0\n",
    "        epoch_focal =0\n",
    "        for images, masks,img_idx in tqdm(train_loader):\n",
    "            images = images.to(config['device'],dtype = torch.float32)\n",
    "            masks = masks.to(config['device'],dtype = torch.float32)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if epoch==0:\n",
    "                    for i in img_idx:\n",
    "                        train_pmap[i.item()] = torch.zeros(images.shape[0],1,256,256)\n",
    "                    p_map = torch.zeros(images.shape[0],1,256,256).to(config['device'])\n",
    "                else:\n",
    "                    p_map= torch.concat([train_pmap[i.item()] for i in img_idx],axis=0).to(config['device'])\n",
    "                input_x = torch.concat([p_map,images],axis=1)\n",
    "                \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_x)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for idx_1,i in enumerate(img_idx):\n",
    "                    train_pmap[i.item()] = torch.sigmoid(outputs[idx_1:idx_1+1,:,:,:])\n",
    "            \n",
    "            dice_loss,focal_loss,jaccard_loss, loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_dice += dice_loss.item()\n",
    "            epoch_focal += focal_loss.item()\n",
    "            epoch_iou += 1-(jaccard_loss.item())\n",
    "            epoch_loss+=loss.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        val_loss,val_iou,val_dice,val_focal,val_pmap = validation(config, model, criterion, valid_loader, epoch,val_pmap)\n",
    "        \n",
    "        es_count += 1\n",
    "        if min_val_loss > val_loss:\n",
    "            es_count = 0\n",
    "            min_val_loss = val_loss\n",
    "            best_model = model\n",
    "            state_dict = model.state_dict()\n",
    "            best_epoch = epoch+1\n",
    "            print(f\"Epoch [{best_epoch}] New Minimum Valid Loss!\")\n",
    "            print(\"..save current best model..\")\n",
    "            model_name = f'epoch {epoch}_current_best_model.pt'\n",
    "            torch.save(state_dict, save_paths['model']+'/'+model_name)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if es_count == config['early_stopping']:\n",
    "            print(f\"Early Stopping Count에 도달했습니다!\")\n",
    "            print(f\"Epoch {epoch+1}, Best Epoch: {best_epoch}, ES Count: {es_count}\\nTrain IoU: {epoch_iou/len(train_loader):6f}, Train Loss: {(epoch_loss/len(train_loader)):6f}, Train Dice: {epoch_dice/len(train_loader):6f}, Train Focal: {epoch_focal/len(train_loader):6f}, Train Jaccard: {((1-epoch_iou)/len(train_loader)):6f}\\nValid_IoU: {val_iou:6f}, Valid Loss: {val_loss:6f}, Valid_Dice: {val_dice:6f}, Valid_Focal: {val_focal:6f}, Valid Jaccard: {(1-val_iou):6f}\")\n",
    "            print(\"***TRAINING DONE***\")\n",
    "            return best_model\n",
    "        print(f\"Epoch {epoch+1}, Best Epoch: {best_epoch}, ES Count: {es_count}\\nTrain IoU: {epoch_iou/len(train_loader):6f}, Train Loss: {(epoch_loss/len(train_loader)):6f}, Train Dice: {epoch_dice/len(train_loader):6f}, Train Focal: {epoch_focal/len(train_loader):6f}, Train Jaccard: {((1-epoch_iou)/len(train_loader)):6f}\\nValid_IoU: {val_iou:6f}, Valid Loss: {val_loss:6f}, Valid_Dice: {val_dice:6f}, Valid_Focal: {val_focal:6f}, Valid Jaccard: {(1-val_iou):6f}\")\n",
    "        print(\"------------------------------------------------------------------------------------\")\n",
    "    print(f\"Early Stopping Count에 도달하지 않았습니다! \\nEarly Stopping Count: {config['early_stopping']} Best Epoch: {best_epoch}\")\n",
    "    print(\"***TRAINING DONE***\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(config['seed']) # 결과 재현용\n",
    "best_model = train(config,model,train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.CWUnet import CWUnet\n",
    "infer_model = CWUnet()\n",
    "infer_model.load_state_dict(torch.load('./logs/models/epoch 6_current_best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(data_paths['test_csv'])\n",
    "test_dataset = Wildfire_Dataset(test_csv,transform_f=base_transforms['test'],infer=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = torchvision.transforms.Resize(256)\n",
    "def inference(model):\n",
    "    model.to(config['device'])\n",
    "    model.eval()\n",
    "    y_pred_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for idx,img in tqdm(enumerate(test_loader)):\n",
    "            img = img.to(config['device'],dtype = torch.float32)\n",
    "            \n",
    "            # pytorch smp 라이브러리 모델은 predict 가능\n",
    "            p_map = torch.zeros(img.shape[0],1,256,256).to(config['device'])\n",
    "            input_x = torch.concat([p_map,img],axis=1)\n",
    "            y_pred = model(input_x)\n",
    "            \n",
    "            # 구현한 FCN은 그냥 output 찍는쪽으로\n",
    "            # y_pred = model(img)\n",
    "            y_pred = resize(y_pred)\n",
    "            y_pred = np.where(y_pred[0, 0, :, :].to('cpu') > config['inference']['threshold'], 1, 0) # 임계값 처리\n",
    "            y_pred = y_pred.astype(np.uint8)\n",
    "            y_pred_dict[test_csv.loc[idx]['test_img']] = y_pred\n",
    "\n",
    "        joblib.dump(y_pred_dict, './y_pred.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = torchvision.transforms.Resize(256)\n",
    "\n",
    "def inference_patch(model):\n",
    "    model.to(config['device'])\n",
    "    model.eval()\n",
    "    y_pred_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for idx,img in tqdm(enumerate(test_loader)):\n",
    "            img = img.to(config['device'],dtype = torch.float32)\n",
    "            \n",
    "            # pytorch smp 라이브러리 모델은 predict 가능\n",
    "            p_map = torch.zeros(img.shape[0],1,256,256).to(config['device'])\n",
    "            patch1 = img[:,:,:256,:256] # 2\n",
    "            patch2 = img[:,:,256:,:256] # 1\n",
    "            patch3 = img[:,:,256:,256:] # 4\n",
    "            patch4 = img[:,:,:256,256:] # 3\n",
    "            patch1 = torch.concat([p_map,patch1],dim=1)\n",
    "            patch2 = torch.concat([p_map,patch2],dim=1)\n",
    "            patch3 = torch.concat([p_map,patch3],dim=1)\n",
    "            patch4 = torch.concat([p_map,patch4],dim=1)\n",
    "            \n",
    "            input_x = torch.concat([patch1,patch2,patch3,patch4],dim=0)\n",
    "            y_pred = model(input_x)\n",
    "            \n",
    "            y_12 = torch.concat([input_x[0:1,:,:,:],input_x[3:4,:,:,:]],dim=3)\n",
    "            y_34 = torch.concat([input_x[1:2,:,:,:],input_x[2:3,:,:,:]],dim=3)\n",
    "            y_pred = torch.concat([y_12,y_34],dim=2)\n",
    "            \n",
    "            y_pred = resize(y_pred)\n",
    "            y_pred = np.where(y_pred[0, 0, :, :].to('cpu') > config['inference']['threshold'], 1, 0) # 임계값 처리\n",
    "            y_pred = y_pred.astype(np.uint8)\n",
    "            y_pred_dict[test_csv.loc[idx]['test_img']] = y_pred\n",
    "\n",
    "        joblib.dump(y_pred_dict, './y_pred.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(config['seed'])\n",
    "inference_patch(infer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_to_check = 0\n",
    "num_imgs_to_show = 4\n",
    "valid_save_path = save_paths['val']+f'epoch_{epoch_to_check}.pkl'\n",
    "val_preds = joblib.load(valid_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = joblib.load(valid_save_path)\n",
    "fig,axs = plt.subplots(num_imgs_to_show,len(config['channels'])+2)\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(20)\n",
    "for i in range(len(config['channels'])):\n",
    "    axs[0,i].set_title('channel'+ str(config['channels'][i]))\n",
    "axs[0,i+1].set_title('label')\n",
    "axs[0,i+2].set_title('pred')\n",
    "cnt=0\n",
    "for idx,(imgs, masks,img_idx) in enumerate(valid_loader):\n",
    "    for i in range(len(imgs)):\n",
    "        iou = iou_pytorch(torch.from_numpy(val_preds[str(idx*config['batch_size']+i)]).unsqueeze(0).unsqueeze(0),masks[i].unsqueeze(0),threshold=config['inference']['threshold'])\n",
    "        for j in range(len(config['channels'])):\n",
    "            axs[i,j].imshow(imgs[i,j,:,:],cmap='gray')\n",
    "        axs[i,-3].set_xlabel(f'iou: {iou.item():6g}')\n",
    "        axs[i,-2].imshow(masks[i,0,:,:],cmap='gray')\n",
    "        axs[i,-2].set_xlabel(str(masks[i,0,:,:].sum().sum().item()))\n",
    "        axs[i,4].imshow(val_preds[str(idx*config['batch_size']+i)],cmap='gray')\n",
    "        axs[i,4].set_xlabel(str(val_preds[str(idx*config['batch_size']+i)].sum().sum()))\n",
    "        cnt+=1\n",
    "        if cnt==num_imgs_to_show:\n",
    "            break\n",
    "    if cnt==num_imgs_to_show:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs_to_show = 4\n",
    "test_save_path = './y_pred.pkl'\n",
    "test_preds = joblib.load(test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = joblib.load(test_save_path)\n",
    "fig,axs = plt.subplots(num_imgs_to_show,len(config['channels'])+1)\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(20)\n",
    "for i in range(len(config['channels'])):\n",
    "    axs[0,i].set_title('channel'+ str(config['channels'][i]))\n",
    "axs[0,i+1].set_title('pred')\n",
    "cnt=0\n",
    "for (imgs) in (test_loader):\n",
    "    for j in range(len(config['channels'])):\n",
    "        axs[cnt,j].imshow(imgs[0,j,:,:],cmap='gray')\n",
    "    axs[cnt,-1].imshow(test_preds[test_csv.loc[cnt]['test_img']],cmap='gray')\n",
    "    axs[cnt,-1].set_xlabel(str(test_preds[test_csv.loc[cnt]['test_img']].sum().sum()))\n",
    "    cnt+=1\n",
    "    if cnt==num_imgs_to_show:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
